{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b5735c",
   "metadata": {},
   "source": [
    "## 1. ÌååÏù∏ÌäúÎãù Ï†Ñ ÌÖåÏä§Ìä∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed39f4",
   "metadata": {},
   "source": [
    "### Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Ïö©ÌïòÏó¨ Í∏∞Ï°¥ yolov5 ÌÖåÏä§Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92d4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ace_jungmin/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2024-5-30 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 objects and saved cropped images to output\n",
      "[tensor([[1.21615e+02, 6.71085e+02, 3.87375e+02, 1.13525e+03, 5.45641e-01, 6.50000e+01],\n",
      "        [6.13981e+02, 2.18636e+02, 8.01454e+02, 4.05490e+02, 3.28335e-01, 3.20000e+01],\n",
      "        [1.66597e+02, 2.27034e+02, 2.82906e+02, 4.15628e+02, 8.74798e-02, 6.40000e+01]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # YOLOv5 ÏûëÏùÄ Î™®Îç∏ ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.07  # confidence threshold\n",
    "model.iou = 0.90 # IoU threshold\n",
    "\n",
    "# confÍ∞Ä ÏûëÏùÑÏàòÎ°ù, iouÍ∞Ä ÌÅ¥ÏàòÎ°ù ÎßéÏùÄ bounding box ÏÉùÏÑ±\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding box Ï∂úÎ†•\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image with Bounding Boxes')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï∂úÎ†•\n",
    "for i, crop_img in enumerate(cropped_images):\n",
    "    plt.figure()\n",
    "    plt.imshow(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Cropped Image {i}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccb6a7",
   "metadata": {},
   "source": [
    "### Ïù∏ÌÑ∞ÎÑ∑ÏóêÏÑú Í∞ÄÏ†∏Ïò® Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Ïù¥Ïö©ÌïòÏó¨ Í∏∞Ï°¥ yolov5 ÌÖåÏä§Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "382c4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ace_jungmin/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2024-5-30 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 objects and saved cropped images to output\n",
      "[tensor([[1.14554e+02, 7.64028e+02, 7.35708e+02, 8.62259e+02, 5.11825e-01, 6.60000e+01],\n",
      "        [3.14305e+02, 3.06990e+01, 6.36716e+02, 2.78095e+02, 2.53351e-01, 4.30000e+01]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # YOLOv5 ÏûëÏùÄ Î™®Îç∏ ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.9 # IoU threshold\n",
    "\n",
    "# confÍ∞Ä ÏûëÏùÑÏàòÎ°ù, iouÍ∞Ä ÌÅ¥ÏàòÎ°ù ÎßéÏùÄ bounding box ÏÉùÏÑ±\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding box Ï∂úÎ†•\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image with Bounding Boxes')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï∂úÎ†•\n",
    "for i, crop_img in enumerate(cropped_images):\n",
    "    plt.figure()\n",
    "    plt.imshow(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Cropped Image {i}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de147f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8eef32",
   "metadata": {},
   "source": [
    "## 2. ÌååÏù∏ÌäúÎãù ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfa8f1",
   "metadata": {},
   "source": [
    "### training data (.png) Î≥µÏÇ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a69e27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î≥µÏÇ¨ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Í∏∞Ï°¥ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "source_dir = '/data_2/ace_minjae/Pill_Dataset/data/Training/source_data/pill_combination_5000/TS_1'\n",
    "target_dir = '/data_2/ace_jungmin/finetune_dataset/'\n",
    "\n",
    "# Í∏∞Ï°¥ Í≤ΩÎ°úÏóêÏÑú Î™®Îì† Ìè¥ÎçîÎ•º ÌÉêÏÉâ\n",
    "for folder_name in os.listdir(source_dir):\n",
    "    folder_path = os.path.join(source_dir, folder_name)\n",
    "    \n",
    "    # Ìè¥ÎçîÏù∏ Í≤ΩÏö∞\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Ìè¥Îçî ÎÇ¥ ÌååÏùºÏùÑ ÌÉêÏÉâ\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # ÌååÏùº Ïù¥Î¶ÑÏóê '90_000_200.png'Í∞Ä Ìè¨Ìï®Îêú Í≤ΩÏö∞\n",
    "            if '90_000_200.png' in filename:\n",
    "                # ÏÉàÎ°úÏö¥ Ìè¥Îçî ÏÉùÏÑ± Î∞è Ìï¥Îãπ ÌååÏùº Î≥µÏÇ¨\n",
    "                new_folder_name = f\"{folder_name}\"\n",
    "                new_folder_path = os.path.join(target_dir, new_folder_name)\n",
    "                os.makedirs(new_folder_path, exist_ok=True)\n",
    "                shutil.copy(os.path.join(folder_path, filename), new_folder_path)\n",
    "\n",
    "print(\"Î≥µÏÇ¨ ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddd176",
   "metadata": {},
   "source": [
    "### label data (.json) Î≥µÏÇ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc415f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î≥µÏÇ¨ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def copy_json_files(source_dir, target_dir):\n",
    "    # ÎåÄÏÉÅ Í≤ΩÎ°ú ÎÇ¥Ïùò Î™®Îì† Ìè¥Îçî ÌôïÏù∏\n",
    "    target_folders = os.listdir(target_dir)\n",
    "\n",
    "    # Í∏∞Ï°¥ Í≤ΩÎ°úÏóêÏÑú Î™®Îì† Ìè¥ÎçîÎ•º ÌÉêÏÉâ\n",
    "    for folder_name in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder_name)\n",
    "        \n",
    "        # Ìè¥ÎçîÍ∞Ä ÏïÑÎãàÍ±∞ÎÇò .zip ÌååÏùºÏù∏ Í≤ΩÏö∞ Í±¥ÎÑàÎúÅÎãàÎã§.\n",
    "        if not os.path.isdir(folder_path) or folder_name.endswith('.zip'):\n",
    "            continue\n",
    "\n",
    "        # Ìè¥Îçî ÎÇ¥Ïóê .json ÌååÏùºÏù¥ ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "        for root, _, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.json') and '90_000_200' in filename:\n",
    "                    source_file_path = os.path.join(root, filename)\n",
    "                    \n",
    "                    # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ìè¥Îçî Ï∞æÍ∏∞\n",
    "                    max_similarity = -1\n",
    "                    similar_folder = None\n",
    "                    for target_folder in target_folders:\n",
    "                        similarity = SequenceMatcher(None, folder_name, target_folder).ratio()\n",
    "                        if similarity > max_similarity:\n",
    "                            max_similarity = similarity\n",
    "                            similar_folder = target_folder\n",
    "                    \n",
    "                    # ÎåÄÏÉÅ Ìè¥ÎçîÏóê .json ÌååÏùº Î≥µÏÇ¨\n",
    "                    target_folder_path = os.path.join(target_dir, similar_folder)\n",
    "                    os.makedirs(target_folder_path, exist_ok=True)\n",
    "                    \n",
    "                    # ÌååÏùº Ïù¥Î¶Ñ Ï§ëÎ≥µ Ï≤òÎ¶¨\n",
    "                    base_name, ext = os.path.splitext(filename)\n",
    "                    target_file_path = os.path.join(target_folder_path, filename)\n",
    "                    count = 1\n",
    "                    while os.path.exists(target_file_path):\n",
    "                        target_file_path = os.path.join(target_folder_path, f\"{base_name}_{count}{ext}\")\n",
    "                        count += 1\n",
    "\n",
    "                    shutil.copy(source_file_path, target_file_path)\n",
    "\n",
    "    print(\"Î≥µÏÇ¨ ÏôÑÎ£å\")\n",
    "\n",
    "# Í∏∞Ï°¥ Í≤ΩÎ°úÏôÄ ÎåÄÏÉÅ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "source_dir = '/data_2/ace_minjae/Pill_Dataset/data/Training/labeled_data/pill_combination_5000'\n",
    "target_dir = '/data_2/ace_jungmin/finetune_dataset/'\n",
    "\n",
    "# .json ÌååÏùº Î≥µÏÇ¨ ÏàòÌñâ\n",
    "copy_json_files(source_dir, target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d2d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72008fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎßàÏßÄÎßâ 2Í∞ú Ìè¥Îçî (021026, 027926ÏùÄ jsonÌååÏùºÏù¥ ÏóÜÏñ¥ png ÌååÏùºÎßå ÏÉùÏÑ±ÎêòÎØÄÎ°ú Îëê Ìè¥ÎçîÎäî ÏàòÏûëÏóÖÏúºÎ°ú Ï†úÍ±∞ ÌïÑÏöî)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfeddc",
   "metadata": {},
   "source": [
    "### yolo Ìè¨Îß∑ÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ÏÖã Î≥ÄÌôò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "375dfef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Îç∞Ïù¥ÌÑ∞ÏÖã Î≥ÄÌôò Î∞è Î∂ÑÌï† ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "source_dir = '/data_2/ace_jungmin/finetune_dataset'\n",
    "target_dir = '/data_2/ace_jungmin/yolo_dataset'\n",
    "\n",
    "# ÌÉÄÍ≤ü ÎîîÎ†âÌÜ†Î¶¨ Ï¥àÍ∏∞Ìôî\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "os.makedirs(target_dir)\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ Ï†ïÏùò (Ïó¨Í∏∞ÏÑúÎäî ÏïΩ ÌïòÎÇòÏùò ÌÅ¥ÎûòÏä§Îßå Ï°¥Ïû¨ÌïúÎã§Í≥† Í∞ÄÏ†ï)\n",
    "classes = ['pill']\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄÏôÄ Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò Î≥ÄÌôò\n",
    "images = []\n",
    "annotations = {}\n",
    "\n",
    "for folder_name in os.listdir(source_dir):\n",
    "    folder_path = os.path.join(source_dir, folder_name)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.png'):\n",
    "            # Ïù¥ÎØ∏ÏßÄ ÌååÏùº ÏàòÏßë\n",
    "            images.append((folder_path, filename))\n",
    "\n",
    "        elif filename.endswith('.json'):\n",
    "            # Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò ÌååÏùº ÏàòÏßë\n",
    "            json_file_path = os.path.join(folder_path, filename)\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if 'images' not in data or 'annotations' not in data:\n",
    "                    continue\n",
    "\n",
    "                for image_info in data['images']:\n",
    "                    img_filename = image_info['file_name']\n",
    "                    annotations[img_filename] = {\n",
    "                        'width': image_info['width'],\n",
    "                        'height': image_info['height'],\n",
    "                        'annotations': [ann['bbox'] for ann in data['annotations'] if ann['image_id'] == image_info['id']]\n",
    "                    }\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄÏôÄ Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò ÏåçÏùÑ ÌõàÎ†®Í≥º Í≤ÄÏ¶ùÏúºÎ°ú Î∂ÑÌï†\n",
    "train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "# ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(target_dir, 'images', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(target_dir, 'labels', split), exist_ok=True)\n",
    "\n",
    "# ÌååÏùº Î≥µÏÇ¨ Î∞è Î≥ÄÌôò\n",
    "def process_split(images, split):\n",
    "    for folder_path, filename in images:\n",
    "        # Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ï≤òÎ¶¨\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img_output_path = os.path.join(target_dir, 'images', split, filename)\n",
    "        shutil.copy(img_path, img_output_path)\n",
    "\n",
    "        # Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò ÌååÏùº Ï≤òÎ¶¨\n",
    "        if filename in annotations:\n",
    "            txt_output_path = os.path.join(target_dir, 'labels', split, filename.replace('.png', '.txt'))\n",
    "            with open(txt_output_path, 'w') as txt_file:\n",
    "                ann_data = annotations[filename]\n",
    "                image_width = ann_data['width']\n",
    "                image_height = ann_data['height']\n",
    "                for bbox in ann_data['annotations']:\n",
    "                    if len(bbox) != 4:\n",
    "                        continue\n",
    "                    x_center = (bbox[0] + bbox[2] / 2) / image_width\n",
    "                    y_center = (bbox[1] + bbox[3] / 2) / image_height\n",
    "                    width = bbox[2] / image_width\n",
    "                    height = bbox[3] / image_height\n",
    "                    class_id = classes.index('pill')\n",
    "                    txt_file.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "# ÌõàÎ†® Î∞è Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖã Ï≤òÎ¶¨\n",
    "process_split(train_images, 'train')\n",
    "process_split(val_images, 'val')\n",
    "\n",
    "print(\"Îç∞Ïù¥ÌÑ∞ÏÖã Î≥ÄÌôò Î∞è Î∂ÑÌï† ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de22a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96fd50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning Î™ÖÎ†πÏñ¥ (yolov5 Ìè¥Îçî ÏïàÏóêÏÑú Ïã§Ìñâ)\n",
    "# python train.py --img 640 --batch 16 --epochs 50 --data ./dataset.yaml --cfg models/yolov5s.yaml --weights yolov5s.pt --name yolov5_finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef434f5",
   "metadata": {},
   "source": [
    "## 3. finetuning Îêú Î™®Îç∏ ÌÖåÏä§Ìä∏ (50 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iouÎ•º Ï§ÑÏù¥Î©¥ Ïó¨Îü¨ Ï§ëÎ≥µÎêòÎäî Î∞ïÏä§ Ï§ÑÏù¥Í∏∞ Í∞ÄÎä• (Î∞òÎåÄÎ°ú, Ïò¨Î¶¨Î©¥ Îçî ÎßéÏùÄ Î∞ïÏä§ ÏÉùÏÑ±)\n",
    "# confÎ•º Ï§ÑÏù¥Î©¥ Îçî ÎßéÏùÄ Í∞ùÏ≤¥ Í≤ÄÏ∂ú (Î∞òÎåÄÎ°ú, Ïò¨Î¶¨Î©¥ ÏùºÎ∂Ä ÌôïÏã§Ìïú Í∞ùÏ≤¥ÎßåÏùÑ Í≤ÄÏ∂ú)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591bcb6",
   "metadata": {},
   "source": [
    "### last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last.pt (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd1c4479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.54798e+02, 2.20745e+02, 2.91772e+02, 4.15497e+02, 2.52580e-01, 0.00000e+00],\n",
      "        [6.06026e+02, 2.14846e+02, 8.14436e+02, 4.08767e+02, 2.37732e-01, 0.00000e+00],\n",
      "        [5.61020e+02, 9.14070e+02, 8.41884e+02, 1.06283e+03, 2.18377e-01, 0.00000e+00],\n",
      "        [1.03765e+02, 6.66417e+02, 4.06939e+02, 1.12151e+03, 1.81983e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bb5ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.57922e+02, 2.52122e+02, 3.55732e+02, 3.74181e+02, 2.70039e-01, 0.00000e+00],\n",
      "        [1.00598e+02, 6.64526e+02, 3.98593e+02, 1.12238e+03, 1.80045e-01, 0.00000e+00],\n",
      "        [6.00247e+02, 1.65613e+02, 8.15237e+02, 5.13401e+02, 9.46224e-02, 0.00000e+00],\n",
      "        [6.03788e+02, 6.97222e+02, 8.33975e+02, 1.13354e+03, 8.84924e-02, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfc4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last.pt (internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a15d2bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[6.62068e+02, 2.98144e+02, 7.64301e+02, 4.17173e+02, 2.63831e-01, 0.00000e+00],\n",
      "        [5.39902e+02, 2.28928e+02, 6.75020e+02, 3.32750e+02, 2.25465e-01, 0.00000e+00],\n",
      "        [5.13625e+02, 3.40502e+02, 6.34932e+02, 4.73498e+02, 2.08208e-01, 0.00000e+00],\n",
      "        [6.08191e+02, 4.33713e+02, 7.50088e+02, 5.76933e+02, 1.48993e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last.pt (real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc39365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 objects and saved cropped images to output\n",
      "[tensor([[3.20546e+03, 2.32896e+03, 4.01018e+03, 2.77666e+03, 2.68888e-01, 0.00000e+00],\n",
      "        [2.30268e+03, 2.68456e+03, 3.05827e+03, 3.33392e+03, 2.48416e-01, 0.00000e+00],\n",
      "        [0.00000e+00, 1.63104e+03, 2.19258e+03, 4.43057e+03, 1.86985e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pill.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337350f",
   "metadata": {},
   "source": [
    "### best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best.pt (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67635790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.54798e+02, 2.20745e+02, 2.91772e+02, 4.15497e+02, 2.52580e-01, 0.00000e+00],\n",
      "        [6.06026e+02, 2.14846e+02, 8.14436e+02, 4.08767e+02, 2.37732e-01, 0.00000e+00],\n",
      "        [5.61020e+02, 9.14070e+02, 8.41884e+02, 1.06283e+03, 2.18377e-01, 0.00000e+00],\n",
      "        [1.03765e+02, 6.66417e+02, 4.06939e+02, 1.12151e+03, 1.81983e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38dc5cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.57922e+02, 2.52122e+02, 3.55732e+02, 3.74181e+02, 2.70039e-01, 0.00000e+00],\n",
      "        [1.00598e+02, 6.64526e+02, 3.98593e+02, 1.12238e+03, 1.80045e-01, 0.00000e+00],\n",
      "        [6.00247e+02, 1.65613e+02, 8.15237e+02, 5.13401e+02, 9.46224e-02, 0.00000e+00],\n",
      "        [6.03788e+02, 6.97222e+02, 8.33975e+02, 1.13354e+03, 8.84924e-02, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a030d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best.pt (internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92d8eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[6.62068e+02, 2.98144e+02, 7.64301e+02, 4.17173e+02, 2.63831e-01, 0.00000e+00],\n",
      "        [5.39902e+02, 2.28928e+02, 6.75020e+02, 3.32750e+02, 2.25465e-01, 0.00000e+00],\n",
      "        [5.13625e+02, 3.40502e+02, 6.34932e+02, 4.73498e+02, 2.08208e-01, 0.00000e+00],\n",
      "        [6.08191e+02, 4.33713e+02, 7.50088e+02, 5.76933e+02, 1.48993e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best.pt (real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8db535d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 objects and saved cropped images to output\n",
      "[tensor([[3.20546e+03, 2.32896e+03, 4.01018e+03, 2.77666e+03, 2.68888e-01, 0.00000e+00],\n",
      "        [2.30268e+03, 2.68456e+03, 3.05827e+03, 3.33392e+03, 2.48416e-01, 0.00000e+00],\n",
      "        [0.00000e+00, 1.63104e+03, 2.19258e+03, 4.43057e+03, 1.86985e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pill.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a101c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d50eac",
   "metadata": {},
   "source": [
    "## 4. finetuning Îêú Î™®Îç∏ ÌÖåÏä§Ìä∏ (100 epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dddd083",
   "metadata": {},
   "source": [
    "### Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú 100epoch ÏßúÎ¶¨(last, best)Îäî Î™ª Ïç®Î®πÏùÑ ÎìØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1b027",
   "metadata": {},
   "source": [
    "### last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last.pt (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d86875ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.61340e+02, 2.18755e+02, 2.90031e+02, 4.09139e+02, 2.06526e-01, 0.00000e+00],\n",
      "        [6.09580e+02, 2.15501e+02, 8.08310e+02, 4.03997e+02, 1.82913e-01, 0.00000e+00],\n",
      "        [1.09641e+02, 6.67423e+02, 4.04740e+02, 1.12509e+03, 1.48815e-01, 0.00000e+00],\n",
      "        [5.61353e+02, 9.11231e+02, 8.42892e+02, 1.06470e+03, 1.26233e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d241b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 6 objects and saved cropped images to output\n",
      "[tensor([[1.59224e+02, 2.51184e+02, 3.55896e+02, 3.74790e+02, 2.38660e-01, 0.00000e+00],\n",
      "        [1.06223e+02, 6.68533e+02, 3.98078e+02, 1.12253e+03, 1.44981e-01, 0.00000e+00],\n",
      "        [6.98582e+02, 9.94931e+00, 9.76000e+02, 5.22156e+02, 3.06926e-03, 0.00000e+00],\n",
      "        [6.10409e+02, 1.35603e+02, 7.90763e+02, 5.16159e+02, 1.14311e-03, 0.00000e+00],\n",
      "        [7.41027e+02, 6.34506e+02, 9.76000e+02, 1.18101e+03, 1.07652e-03, 0.00000e+00],\n",
      "        [6.25634e+02, 9.22470e+02, 8.15451e+02, 1.14280e+03, 1.00430e-03, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "\n",
    "# ÎÖ∏ÎûëÏù¥ ÌÉêÏßÄ Í±∞Ïùò Î™ªÌï®\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.001 #confidence threshold\n",
    "model.iou = 0.4# IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last.pt (internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43649890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 7 objects and saved cropped images to output\n",
      "[tensor([[5.38164e+02, 2.32113e+02, 6.72929e+02, 3.36097e+02, 1.97684e-01, 0.00000e+00],\n",
      "        [6.61121e+02, 2.94862e+02, 7.65697e+02, 4.21039e+02, 1.71068e-01, 0.00000e+00],\n",
      "        [5.36701e+02, 2.35630e+02, 6.73207e+02, 3.39992e+02, 1.55530e-01, 0.00000e+00],\n",
      "        [5.35082e+02, 2.29543e+02, 6.75192e+02, 3.38269e+02, 1.34881e-01, 0.00000e+00],\n",
      "        [5.35965e+02, 2.27211e+02, 6.65095e+02, 3.41839e+02, 1.32407e-01, 0.00000e+00],\n",
      "        [6.61092e+02, 3.01216e+02, 7.68213e+02, 4.21419e+02, 1.31591e-01, 0.00000e+00],\n",
      "        [6.12309e+02, 4.11879e+02, 7.46984e+02, 6.05674e+02, 1.14099e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.95 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddaa81e",
   "metadata": {},
   "source": [
    "### best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9780b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best.pt (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acaae31d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[6.04107e+02, 2.12291e+02, 8.08121e+02, 4.08658e+02, 2.19981e-01, 0.00000e+00],\n",
      "        [1.58442e+02, 2.15921e+02, 2.89066e+02, 4.09540e+02, 2.17796e-01, 0.00000e+00],\n",
      "        [1.08052e+02, 6.59301e+02, 3.99593e+02, 1.12720e+03, 2.08649e-01, 0.00000e+00],\n",
      "        [5.66394e+02, 9.12175e+02, 8.40141e+02, 1.07029e+03, 1.95756e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.85 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb6dea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.55649e+02, 2.47526e+02, 3.54281e+02, 3.78041e+02, 2.57355e-01, 0.00000e+00],\n",
      "        [1.03048e+02, 6.58561e+02, 3.94218e+02, 1.13054e+03, 2.09920e-01, 0.00000e+00],\n",
      "        [6.12445e+02, 6.75463e+02, 8.22282e+02, 1.15423e+03, 3.91691e-02, 0.00000e+00],\n",
      "        [6.97592e+02, 1.18335e+01, 9.76000e+02, 5.40950e+02, 2.86395e-02, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# Îπ®Í∞ïÏù¥ Í±∞Ïùò Í≤ÄÏ∂ú Î™ªÌï®\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.005  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-010224-016551-027926_0_2_0_2_90_000_200.png'\n",
    "#img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-001900-004543-016551-024850_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best.pt (internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d649752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[5.46121e+02, 2.35909e+02, 6.69479e+02, 3.29004e+02, 1.15332e-01, 0.00000e+00],\n",
      "        [6.60965e+02, 2.96972e+02, 7.62842e+02, 4.21715e+02, 1.12851e-01, 0.00000e+00],\n",
      "        [6.06272e+02, 4.12314e+02, 7.42303e+02, 5.74727e+02, 7.04414e-02, 0.00000e+00],\n",
      "        [5.16991e+02, 3.32449e+02, 6.24657e+02, 4.84158e+02, 4.18843e-02, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÎêòÍ∏¥ ÎêòÎäîÎç∞ confÎ•º 0.004ÍπåÏßÄ ..?\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.004  # confidence threshold\n",
    "model.iou = 0.3 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6c891",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64433b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.15401e+02, 8.10791e+02, 3.63570e+02, 1.06707e+03, 2.59435e-01, 0.00000e+00],\n",
      "        [3.83557e+01, 2.12511e+02, 3.37261e+02, 5.05632e+02, 2.50005e-01, 0.00000e+00],\n",
      "        [6.15585e+02, 6.78445e+02, 8.71526e+02, 1.13062e+03, 2.09687e-01, 0.00000e+00],\n",
      "        [5.23008e+02, 1.17836e+02, 9.30051e+02, 5.69592e+02, 1.42342e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune_100/weights/best.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.85 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "img_path = '/data_2/ace_jungmin/yolo_dataset/images/val/K-000573-023223-025438-037777_0_2_0_2_90_000_200.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd3106ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 objects and saved cropped images to output\n",
      "[tensor([[1.36879e+02, 4.73755e+01, 2.61774e+02, 1.12756e+02, 1.82433e-01, 0.00000e+00],\n",
      "        [3.42711e+02, 0.00000e+00, 3.97806e+02, 1.46718e+02, 1.55188e-01, 0.00000e+00],\n",
      "        [1.20028e+00, 0.00000e+00, 9.22078e+01, 1.85000e+02, 1.51299e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/ty2_pill.jfif'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb88779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 objects and saved cropped images to output\n",
      "[tensor([[1.79945e+03, 9.34147e+02, 2.10070e+03, 1.23134e+03, 2.73959e-01, 0.00000e+00],\n",
      "        [2.49659e+03, 1.45325e+03, 2.85874e+03, 2.08508e+03, 2.47209e-01, 0.00000e+00],\n",
      "        [1.09899e+03, 2.07386e+03, 1.27832e+03, 2.38027e+03, 1.79879e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.1  # confidence threshold\n",
    "model.iou = 0.8 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/my_pill_1.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15837b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 objects and saved cropped images to output\n",
      "[tensor([[1.90709e+03, 1.52898e+03, 2.18752e+03, 1.80494e+03, 2.77548e-01, 0.00000e+00],\n",
      "        [6.66231e+02, 1.23787e+03, 9.52047e+02, 1.43518e+03, 2.58466e-01, 0.00000e+00],\n",
      "        [1.35071e+03, 2.25939e+03, 1.91219e+03, 2.69898e+03, 2.11261e-02, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.01  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/my_pill_2.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0738f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 7 objects and saved cropped images to output\n",
      "[tensor([[1.69818e+03, 4.42906e+02, 2.13002e+03, 8.53053e+02, 2.71195e-01, 0.00000e+00],\n",
      "        [1.43031e+03, 1.88471e+03, 1.93953e+03, 2.31277e+03, 2.50367e-01, 0.00000e+00],\n",
      "        [1.18766e+03, 9.14136e+02, 1.66533e+03, 1.27536e+03, 2.48809e-01, 0.00000e+00],\n",
      "        [1.93940e+03, 1.32333e+03, 2.57733e+03, 1.57947e+03, 2.47027e-01, 0.00000e+00],\n",
      "        [5.86787e+02, 2.07411e+03, 1.03659e+03, 2.52646e+03, 2.45254e-01, 0.00000e+00],\n",
      "        [4.89733e+02, 1.42509e+03, 8.58284e+02, 1.89540e+03, 1.97136e-01, 0.00000e+00],\n",
      "        [8.98139e+02, 1.47721e+03, 1.27917e+03, 1.93513e+03, 1.95725e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pills_1.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'pill_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f005a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 7 objects and saved cropped images to output\n",
      "[tensor([[1.55786e+03, 2.99875e+02, 1.93396e+03, 6.92851e+02, 2.60335e-01, 0.00000e+00],\n",
      "        [1.76611e+03, 1.03984e+03, 2.36713e+03, 1.30620e+03, 2.54467e-01, 0.00000e+00],\n",
      "        [1.11061e+03, 7.28636e+02, 1.54443e+03, 1.08780e+03, 2.41090e-01, 0.00000e+00],\n",
      "        [5.87131e+02, 1.77659e+03, 1.01479e+03, 2.20070e+03, 2.23462e-01, 0.00000e+00],\n",
      "        [1.36470e+03, 1.59598e+03, 1.82413e+03, 1.97218e+03, 2.21974e-01, 0.00000e+00],\n",
      "        [5.14853e+02, 1.21776e+03, 8.41586e+02, 1.62779e+03, 2.11106e-01, 0.00000e+00],\n",
      "        [8.74651e+02, 1.23199e+03, 1.20902e+03, 1.65453e+03, 2.06068e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pills_2.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c03e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 7 objects and saved cropped images to output\n",
      "[tensor([[9.52803e+02, 2.03969e+03, 1.44132e+03, 2.34093e+03, 2.76600e-01, 0.00000e+00],\n",
      "        [3.43815e+02, 1.51250e+03, 8.33817e+02, 1.97335e+03, 2.46789e-01, 0.00000e+00],\n",
      "        [2.11793e+03, 1.34430e+03, 2.51947e+03, 1.68931e+03, 2.46428e-01, 0.00000e+00],\n",
      "        [1.72802e+03, 1.96716e+03, 2.19080e+03, 2.45572e+03, 2.38843e-01, 0.00000e+00],\n",
      "        [1.53633e+03, 1.23401e+03, 1.90252e+03, 1.63463e+03, 2.22495e-01, 0.00000e+00],\n",
      "        [7.11265e+02, 1.10412e+03, 1.13391e+03, 1.44738e+03, 1.65656e-01, 0.00000e+00],\n",
      "        [9.33726e+02, 1.37825e+03, 1.31397e+03, 1.76109e+03, 1.40132e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pills_3.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9814d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.80961e+03, 1.78719e+03, 2.40761e+03, 2.36549e+03, 2.46385e-01, 0.00000e+00],\n",
      "        [6.27932e+02, 1.01813e+03, 1.35510e+03, 1.56179e+03, 2.40730e-01, 0.00000e+00],\n",
      "        [6.68853e+02, 2.29179e+03, 1.49871e+03, 2.95670e+03, 2.31457e-01, 0.00000e+00],\n",
      "        [1.61910e+03, 1.03701e+03, 2.53057e+03, 1.41629e+03, 2.28744e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pills_4.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95d6a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 objects and saved cropped images to output\n",
      "[tensor([[1.83891e+03, 2.36583e+03, 2.45290e+03, 2.97305e+03, 2.60478e-01, 0.00000e+00],\n",
      "        [7.81490e+02, 2.16330e+03, 1.49001e+03, 2.70447e+03, 2.58641e-01, 0.00000e+00],\n",
      "        [1.56711e+03, 1.51560e+03, 2.45947e+03, 1.97687e+03, 2.03188e-01, 0.00000e+00],\n",
      "        [7.79604e+02, 1.06437e+03, 1.79450e+03, 1.75247e+03, 1.96582e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pills_5.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae5e4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 8 objects and saved cropped images to output\n",
      "[tensor([[1.41579e+03, 1.87955e+03, 1.87012e+03, 2.31306e+03, 2.70986e-01, 0.00000e+00],\n",
      "        [1.64558e+03, 1.39294e+03, 2.46093e+03, 1.68367e+03, 2.51984e-01, 0.00000e+00],\n",
      "        [1.05909e+03, 2.65694e+03, 1.64357e+03, 3.14635e+03, 2.51115e-01, 0.00000e+00],\n",
      "        [6.88173e+02, 1.53669e+03, 1.29486e+03, 1.93804e+03, 2.49598e-01, 0.00000e+00],\n",
      "        [2.02267e+03, 2.07156e+03, 2.70256e+03, 2.53005e+03, 2.43004e-01, 0.00000e+00],\n",
      "        [5.20298e+02, 2.26783e+03, 1.37884e+03, 2.72397e+03, 2.35772e-01, 0.00000e+00],\n",
      "        [1.65349e+03, 2.80706e+03, 2.31409e+03, 3.26066e+03, 2.16637e-01, 0.00000e+00],\n",
      "        [1.23537e+03, 9.79904e+02, 1.76083e+03, 1.48844e+03, 2.16609e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5 # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill1.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill2.jpeg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill3.jpg'\n",
    "#img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill4.jfif'\n",
    "img_path = '/home/ace_jungmin/jupyter_notebook/pill_segmentation/real_pills_6.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    raise ValueError(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "results = model(img)\n",
    "\n",
    "# Í≤∞Í≥º ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "shutil.copy(img_path, output_dir / 'original_image.png')\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "img_with_boxes = img.copy()\n",
    "\n",
    "# bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "cropped_images = []\n",
    "for i, bbox in enumerate(results.xyxy):\n",
    "    for j, detection in enumerate(bbox):\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        # bounding box Í∑∏Î¶¨Í∏∞\n",
    "        cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(crop_img)\n",
    "\n",
    "        # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        crop_img_path = output_dir / f'drug_{i}_{j}.png'\n",
    "        cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "\n",
    "# ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "output_image_path = output_dir / 'image_with_boxes.png'\n",
    "cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "\n",
    "print(f\"Detected {len(cropped_images)} objects and saved cropped images to {output_dir}\")\n",
    "\n",
    "print(results.xyxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2073c724",
   "metadata": {},
   "source": [
    "# Fine-Tuning ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "865472ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_08ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.14007e+03, 1.56614e+03, 1.78457e+03, 2.34792e+03, 2.70520e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.28994e+03, 1.14628e+03, 2.19294e+03, 2.28584e+03, 2.19013e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_07ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.13750e+03, 1.87616e+03, 1.84394e+03, 2.61254e+03, 2.74198e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_05ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.84684e+02, 2.05294e+03, 1.81730e+03, 2.72861e+03, 2.42727e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_01ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.09630e+02, 1.61044e+03, 1.98876e+03, 2.55743e+03, 2.64069e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_06ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.34125e+03, 1.72408e+03, 2.24637e+03, 2.42607e+03, 2.46002e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_02ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.00119e+03, 1.71096e+03, 2.03575e+03, 2.44196e+03, 2.51671e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_09ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.17285e+03, 2.20471e+03, 2.04513e+03, 3.17299e+03, 2.56542e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_04ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.45949e+03, 1.68423e+03, 2.31975e+03, 2.45407e+03, 2.55771e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_07ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.00119e+03, 1.71096e+03, 2.03575e+03, 2.44196e+03, 2.51671e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_03ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-024603Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.97593e+02, 1.66946e+03, 1.93424e+03, 2.36611e+03, 2.53872e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_16ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.17745e+03, 1.96820e+03, 2.25281e+03, 2.83334e+03, 2.50218e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_13ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.11882e+03, 1.95638e+03, 2.18251e+03, 2.69222e+03, 2.32146e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_20ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.00381e+03, 1.80663e+03, 1.99185e+03, 2.50134e+03, 2.29688e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_26ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.46162e+03, 1.40173e+03, 2.63678e+03, 2.39045e+03, 2.71421e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_19ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.37195e+02, 1.80233e+03, 1.81843e+03, 2.78806e+03, 2.67646e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_27ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.12377e+03, 1.72183e+03, 2.14316e+03, 2.68213e+03, 2.21584e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_14ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.28481e+03, 2.02547e+03, 2.47651e+03, 2.77809e+03, 2.69603e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_24ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.89780e+02, 1.60653e+03, 2.21758e+03, 2.43442e+03, 2.23074e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_17ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.10882e+03, 1.51503e+03, 2.24112e+03, 2.24775e+03, 2.67390e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_15ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.08533e+03, 1.51168e+03, 2.13128e+03, 2.53341e+03, 2.74127e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_25ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.18005e+03, 1.10260e+03, 2.15130e+03, 2.22753e+03, 2.44683e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_21ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.07540e+03, 1.88763e+03, 2.27625e+03, 2.66891e+03, 2.59648e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_23ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.56843e+02, 1.76933e+03, 2.10104e+03, 2.70170e+03, 2.44545e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_18ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[6.43710e+02, 1.69009e+03, 1.73905e+03, 2.41846e+03, 2.25905e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_22ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033623Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.55759e+02, 1.21246e+03, 2.19789e+03, 2.31776e+03, 2.33245e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_10ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[7.70296e+02, 1.27671e+03, 1.59359e+03, 2.09663e+03, 2.64140e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_18ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.11765e+02, 1.53766e+03, 1.85871e+03, 2.47212e+03, 2.38137e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_11ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.49240e+03, 1.83095e+03, 2.28199e+03, 2.54943e+03, 2.81961e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_09ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.29643e+03, 1.78944e+03, 2.00840e+03, 2.47174e+03, 2.79403e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_14ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.41035e+02, 1.34196e+03, 1.75328e+03, 2.23652e+03, 2.70565e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_13ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.98952e+02, 1.65257e+03, 1.82070e+03, 2.44776e+03, 2.70767e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_15ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.14965e+03, 1.65652e+03, 1.97001e+03, 2.48470e+03, 2.64168e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_16ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.23425e+03, 2.22772e+03, 2.00382e+03, 2.95037e+03, 2.63082e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_12ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.07998e+03, 1.54054e+03, 1.96717e+03, 2.36733e+03, 2.62445e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_19ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.20368e+03, 1.60912e+03, 2.18357e+03, 2.52897e+03, 2.67482e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_17ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-000573Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.99636e+02, 1.95122e+03, 1.93699e+03, 2.87018e+03, 2.48764e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_19ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[5.02296e+02, 1.50885e+03, 1.58068e+03, 2.45967e+03, 2.43301e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_18ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.10971e+03, 1.55047e+03, 2.24379e+03, 2.06941e+03, 2.63166e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_12ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.16370e+03, 1.83950e+03, 2.45318e+03, 2.74050e+03, 2.42990e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_14ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.00296e+03, 1.91997e+03, 2.19089e+03, 2.39218e+03, 2.36190e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_11ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.38061e+02, 1.85524e+03, 2.26191e+03, 2.42755e+03, 2.34653e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_15ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.47934e+02, 1.58654e+03, 2.01374e+03, 2.37350e+03, 2.38815e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_13ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.16223e+03, 1.15398e+03, 2.27187e+03, 2.27822e+03, 2.37046e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_10ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.95911e+02, 1.66802e+03, 2.28241e+03, 2.54639e+03, 2.38369e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_16ÏóêÏÑú 2Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.04969e+03, 1.17403e+03, 2.07479e+03, 2.07473e+03, 2.41115e-01, 0.00000e+00],\n",
      "        [1.01469e+02, 1.60988e+02, 2.02698e+03, 2.17274e+03, 1.14011e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175425515_17ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-037941Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[6.90121e+02, 1.38964e+03, 2.00853e+03, 1.95722e+03, 2.66045e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_06ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[7.67499e+02, 1.23529e+03, 2.30059e+03, 2.35489e+03, 2.43122e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_02ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.81757e+02, 1.92142e+03, 2.51416e+03, 2.56369e+03, 1.78988e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_05ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.79508e+02, 1.52695e+03, 2.44764e+03, 2.50361e+03, 2.55145e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_03ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[4.96736e+02, 1.58274e+03, 1.92188e+03, 2.53553e+03, 1.91673e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_04ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[7.51403e+02, 1.31669e+03, 2.13047e+03, 2.29491e+03, 2.09332e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_07ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.72407e+02, 1.48356e+03, 2.12573e+03, 3.16190e+03, 2.64863e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[7.98514e+02, 1.46612e+03, 2.35350e+03, 2.10966e+03, 1.61065e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_01ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.17215e+03, 1.02772e+03, 2.15409e+03, 2.60590e+03, 1.79609e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_08ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-011441Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.21884e+03, 1.22287e+03, 1.86489e+03, 2.40571e+03, 2.39656e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_06ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[5.17934e+02, 1.64948e+03, 1.25467e+03, 2.75483e+03, 2.61052e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_04ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.11023e+03, 1.59760e+03, 2.09361e+03, 2.30209e+03, 2.48584e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_10ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.38878e+03, 1.61922e+03, 2.28813e+03, 2.73756e+03, 2.66013e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_12ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.75394e+02, 1.35465e+03, 2.06672e+03, 2.24453e+03, 2.74560e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_08ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.22859e+03, 1.51843e+03, 2.27791e+03, 2.31990e+03, 2.59753e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_02ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.41400e+03, 9.99310e+02, 2.15392e+03, 1.89641e+03, 2.55737e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.17131e+03, 1.79373e+03, 2.20925e+03, 2.46463e+03, 2.39381e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_05ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.07668e+03, 1.76673e+03, 2.09938e+03, 2.46072e+03, 2.35215e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_11ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.31729e+03, 1.60284e+03, 2.56340e+03, 2.39693e+03, 2.46403e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_03ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.09026e+03, 1.58931e+03, 2.05203e+03, 2.45648e+03, 2.60690e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_07ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.25914e+03, 1.72162e+03, 2.07547e+03, 2.67346e+03, 2.81135e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_09ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.21933e+03, 1.85087e+03, 1.90696e+03, 2.90143e+03, 2.59377e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175526981_01ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-030590Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.22186e+03, 1.13173e+03, 1.96901e+03, 2.21380e+03, 2.60457e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_02ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.56013e+02, 1.71823e+03, 1.83417e+03, 2.43344e+03, 2.60681e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_11ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.27037e+03, 1.39955e+03, 2.32309e+03, 2.37739e+03, 2.77484e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_08ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.08662e+02, 1.49822e+03, 1.91450e+03, 2.59191e+03, 2.40086e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_01ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.19502e+03, 1.73094e+03, 2.20237e+03, 2.49675e+03, 2.37318e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_06ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.58898e+02, 1.50561e+03, 1.99156e+03, 2.25284e+03, 2.55108e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_09ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.24402e+03, 1.82156e+03, 2.23638e+03, 2.72351e+03, 2.58711e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_03ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.07847e+03, 1.54335e+03, 2.11346e+03, 2.34799e+03, 2.48612e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.27178e+03, 1.51788e+03, 2.20987e+03, 2.43337e+03, 2.60459e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_05ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.83278e+02, 1.55258e+03, 1.94525e+03, 2.40493e+03, 2.41438e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_12ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.07742e+03, 1.63032e+03, 2.12773e+03, 2.42839e+03, 2.30199e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_04ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.27627e+03, 1.62575e+03, 2.29101e+03, 2.52216e+03, 2.30422e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175600495_10ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-033615Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.13759e+03, 2.00642e+03, 2.13238e+03, 2.76779e+03, 2.29550e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_21ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[9.96643e+02, 1.67602e+03, 2.49716e+03, 2.58458e+03, 2.37359e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_24ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[7.03197e+02, 2.01573e+03, 2.13579e+03, 2.80390e+03, 2.05024e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_26ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.15158e+03, 1.49513e+03, 2.49746e+03, 2.26080e+03, 1.77016e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_25ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[8.55099e+02, 1.72537e+03, 2.27064e+03, 2.27529e+03, 1.88116e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_28ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.06829e+03, 1.50872e+03, 2.16903e+03, 2.44984e+03, 2.37798e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_20ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[7.90896e+02, 1.29030e+03, 2.10990e+03, 2.59876e+03, 2.09924e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_23ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.16497e+03, 1.34375e+03, 2.32022e+03, 2.55675e+03, 2.60746e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_27ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[1.10929e+03, 1.68142e+03, 2.10943e+03, 2.59260e+03, 2.59833e-01, 0.00000e+00]], device='cuda:0')]\n",
      "Ïù¥ÎØ∏ÏßÄ KakaoTalk_20240607_175239337_22ÏóêÏÑú 1Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º output/fine_tuning_data/K-025090Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\n",
      "[tensor([[5.62093e+02, 1.53198e+03, 1.69837e+03, 2.39872e+03, 2.41863e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # ÌååÏù∏ÌäúÎãùÌïú Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "\n",
    "# ÌÉêÏßÄ ÏÑ§Ï†ï Ï°∞Ï†ï\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5  # IoU threshold\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏûàÎäî ÏÉÅÏúÑ Ìè¥Îçî Í≤ΩÎ°ú\n",
    "img_base_folder_path = Path('/home/ace_jungmin/jupyter_notebook/pill_segmentation/pill')\n",
    "\n",
    "# Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† ÏÉÅÏúÑ Ìè¥Îçî Í≤ΩÎ°ú\n",
    "output_base_dir = Path('output/fine_tuning_data')\n",
    "\n",
    "# Î™®Îì† jpg ÌååÏùºÏóê ÎåÄÌï¥ ÌÉêÏßÄ ÏàòÌñâ\n",
    "for img_path in img_base_folder_path.rglob('*.jpg'):\n",
    "    # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "    img = cv2.imread(str(img_path))\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï Î∞è Î™®Îç∏Ïóê ÎßûÍ≤å Ï†ÑÏ≤òÎ¶¨\n",
    "    results = model(img)\n",
    "\n",
    "    # Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏÜçÌï¥ÏûàÎçò Ìè¥ÎçîÏùò Íµ¨Ï°∞Î•º Ïú†ÏßÄÌïòÎ©¥ÏÑú ÏÉàÎ°úÏö¥ Ìè¥Îçî ÏÉùÏÑ±\n",
    "    relative_path = img_path.relative_to(img_base_folder_path)\n",
    "    output_dir = output_base_dir / relative_path.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "    #shutil.copy(str(img_path), output_dir / f'original_image_{img_path.stem}.png')\n",
    "\n",
    "    # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏÇ¨ÌïòÏó¨ Í≤∞Í≥ºÎ•º Í∑∏Î¶¥ Ï§ÄÎπÑ\n",
    "    img_with_boxes = img.copy()\n",
    "\n",
    "    # bounding box Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞ Î∞è Ï†ÄÏû•\n",
    "    cropped_images = []\n",
    "    for i, bbox in enumerate(results.xyxy):\n",
    "        for j, detection in enumerate(bbox):\n",
    "            x1, y1, x2, y2, conf, cls = detection\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "            # bounding box Í∑∏Î¶¨Í∏∞\n",
    "            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # ÏïΩ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥Í∏∞\n",
    "            crop_img = img[y1:y2, x1:x2]\n",
    "            cropped_images.append(crop_img)\n",
    "\n",
    "            # ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "            crop_img_path = output_dir / f'drug_{img_path.stem}_{i}_{j}.png'\n",
    "            cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "    # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉêÏßÄÎêú bounding boxÍ∞Ä Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "    #output_image_path = output_dir / f'image_with_boxes_{img_path.stem}.png'\n",
    "    #cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "    print(f\"Ïù¥ÎØ∏ÏßÄ {img_path.stem}ÏóêÏÑú {len(cropped_images)}Í∞úÏùò Í∞ùÏ≤¥Î•º ÌÉêÏßÄÌïòÏòÄÏúºÎ©∞, ÏûòÎùºÎÇ∏ Ïù¥ÎØ∏ÏßÄÎ•º {output_dir}Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\")\n",
    "\n",
    "    print(results.xyxy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
