{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6900386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ìœ„í•œ ë°ì´í„° crop ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc21068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['requests>=2.32.0'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âŒ AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ğŸš€ v7.0-317-g00403794 Python-3.9.18 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë¯¸ì§€ pill_denseì—ì„œ 5ê°œì˜ ê°ì²´ë¥¼ íƒì§€í•˜ì˜€ìœ¼ë©°, ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ crop_test_images/pill_denseì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "[tensor([[5.25416e+02, 1.55215e+03, 1.07367e+03, 2.30211e+03, 1.72188e-01, 0.00000e+00],\n",
      "        [1.22863e+03, 1.18925e+03, 1.87863e+03, 1.68791e+03, 1.65858e-01, 0.00000e+00],\n",
      "        [1.90952e+03, 1.72476e+03, 2.65832e+03, 2.27492e+03, 1.24853e-01, 0.00000e+00],\n",
      "        [1.74706e+03, 2.26210e+03, 2.31021e+03, 3.03017e+03, 7.87285e-02, 0.00000e+00],\n",
      "        [1.69722e+03, 1.54785e+03, 2.23023e+03, 2.05435e+03, 5.47572e-02, 0.00000e+00]], device='cuda:0')]\n",
      "ì´ë¯¸ì§€ pill_blackì—ì„œ 8ê°œì˜ ê°ì²´ë¥¼ íƒì§€í•˜ì˜€ìœ¼ë©°, ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ crop_test_images/pill_blackì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "[tensor([[1.62967e+03, 4.57429e+02, 2.06152e+03, 7.86759e+02, 2.56675e-01, 0.00000e+00],\n",
      "        [9.33215e+02, 1.32040e+03, 1.21948e+03, 1.86289e+03, 2.54605e-01, 0.00000e+00],\n",
      "        [1.42257e+03, 2.35289e+03, 1.96719e+03, 2.79877e+03, 2.53860e-01, 0.00000e+00],\n",
      "        [2.01987e+03, 1.36523e+03, 2.44201e+03, 1.72109e+03, 2.40580e-01, 0.00000e+00],\n",
      "        [1.14018e+03, 6.40456e+02, 1.50619e+03, 1.03423e+03, 2.18035e-01, 0.00000e+00],\n",
      "        [1.33076e+03, 3.01886e+03, 1.68876e+03, 3.39217e+03, 2.16133e-01, 0.00000e+00],\n",
      "        [1.53827e+03, 1.02562e+03, 1.99373e+03, 1.32190e+03, 1.68058e-01, 0.00000e+00],\n",
      "        [1.85194e+03, 1.62765e+03, 2.17134e+03, 2.09522e+03, 1.11984e-01, 0.00000e+00]], device='cuda:0')]\n",
      "ì´ë¯¸ì§€ pill_whiteì—ì„œ 8ê°œì˜ ê°ì²´ë¥¼ íƒì§€í•˜ì˜€ìœ¼ë©°, ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ crop_test_images/pill_whiteì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "[tensor([[5.00546e+02, 1.13127e+03, 1.24972e+03, 1.55839e+03, 2.63092e-01, 0.00000e+00],\n",
      "        [1.42529e+03, 2.27077e+03, 2.10223e+03, 3.03544e+03, 2.50098e-01, 0.00000e+00],\n",
      "        [1.95340e+03, 1.22009e+03, 2.58204e+03, 1.75210e+03, 2.38788e-01, 0.00000e+00],\n",
      "        [7.78944e+02, 2.18095e+03, 1.33599e+03, 2.87515e+03, 2.36639e-01, 0.00000e+00],\n",
      "        [1.51474e+03, 5.49521e+02, 2.04773e+03, 1.20464e+03, 2.30198e-01, 0.00000e+00],\n",
      "        [2.34055e+03, 1.74661e+03, 2.82403e+03, 2.47260e+03, 2.22295e-01, 0.00000e+00],\n",
      "        [1.22255e+03, 1.56018e+03, 2.11902e+03, 2.05968e+03, 2.10619e-01, 0.00000e+00],\n",
      "        [4.57164e+02, 1.69995e+03, 1.00909e+03, 2.32446e+03, 2.02402e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_weights_path = '/data_2/ace_jungmin/yolov5/runs/train/yolov5_finetune/weights/last.pt'\n",
    "model = torch.hub.load('/data_2/ace_jungmin/yolov5', 'custom', path=model_weights_path, source='local')  # íŒŒì¸íŠœë‹í•œ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
    "\n",
    "# íƒì§€ ì„¤ì • ì¡°ì •\n",
    "model.conf = 0.05  # confidence threshold\n",
    "model.iou = 0.5  # IoU threshold\n",
    "\n",
    "# ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒìœ„ í´ë” ê²½ë¡œ\n",
    "img_base_folder_path = Path('/home/ace_jungmin/jupyter_notebook/pill_segmentation/test_images')\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ìƒìœ„ í´ë” ê²½ë¡œ\n",
    "output_base_dir = Path('crop_test_images')\n",
    "\n",
    "# ëª¨ë“  jpg íŒŒì¼ì— ëŒ€í•´ íƒì§€ ìˆ˜í–‰\n",
    "for img_path in img_base_folder_path.rglob('*.jpg'):\n",
    "    # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "    img = cv2.imread(str(img_path))\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ë° ëª¨ë¸ì— ë§ê²Œ ì „ì²˜ë¦¬\n",
    "    results = model(img)\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ í´ë” ìƒì„±\n",
    "    image_name = img_path.stem  # ì´ë¯¸ì§€ì˜ íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ë¶€ë¶„\n",
    "    output_dir = output_base_dir / image_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥\n",
    "    shutil.copy(str(img_path), output_dir / f'original_image_{image_name}.png')\n",
    "\n",
    "    # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³µì‚¬í•˜ì—¬ ê²°ê³¼ë¥¼ ê·¸ë¦´ ì¤€ë¹„\n",
    "    img_with_boxes = img.copy()\n",
    "\n",
    "    # bounding box ê¸°ë°˜ ì´ë¯¸ì§€ ìë¥´ê¸° ë° ì €ì¥\n",
    "    cropped_images = []\n",
    "    for i, bbox in enumerate(results.xyxy):\n",
    "        for j, detection in enumerate(bbox):\n",
    "            x1, y1, x2, y2, conf, cls = detection\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "            # bounding box ê·¸ë¦¬ê¸°\n",
    "            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # ì•½ ì´ë¯¸ì§€ ìë¥´ê¸°\n",
    "            crop_img = img[y1:y2, x1:x2]\n",
    "            cropped_images.append(crop_img)\n",
    "\n",
    "            # ì˜ë¼ë‚¸ ì´ë¯¸ì§€ ì €ì¥\n",
    "            crop_img_path = output_dir / f'drug_{image_name}_{i}_{j}.png'\n",
    "            cv2.imwrite(str(crop_img_path), crop_img)\n",
    "\n",
    "    # ì›ë³¸ ì´ë¯¸ì§€ì™€ íƒì§€ëœ bounding boxê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€ ì €ì¥\n",
    "    output_image_path = output_dir / f'image_with_boxes_{image_name}.png'\n",
    "    cv2.imwrite(str(output_image_path), img_with_boxes)\n",
    "\n",
    "    print(f\"ì´ë¯¸ì§€ {image_name}ì—ì„œ {len(cropped_images)}ê°œì˜ ê°ì²´ë¥¼ íƒì§€í•˜ì˜€ìœ¼ë©°, ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ {output_dir}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    print(results.xyxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
